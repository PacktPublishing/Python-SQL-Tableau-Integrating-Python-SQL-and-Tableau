{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a logistic regression to predict absenteeism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed CSV data\n",
    "data_preprocessed = pd.read_csv('Absenteeism_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eyeball the data\n",
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the median of 'Absenteeism Time in Hours'\n",
    "data_preprocessed['Absenteeism Time in Hours'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets for our logistic regression\n",
    "# they have to be categories and we must find a way to say if someone is 'being absent too much' or not\n",
    "# what we've decided to do is to take the median of the dataset as a cut-off line\n",
    "# in this way the dataset will be balanced (there will be roughly equal number of 0s and 1s for the logistic regression)\n",
    "# as balancing is a great problem for ML, this will work great for us\n",
    "# alternatively, if we had more data, we could have found other ways to deal with the issue \n",
    "# for instance, we could have assigned some arbitrary value as a cut-off line, instead of the median\n",
    "\n",
    "# note that what line does is to assign 1 to anyone who has been absent 4 hours or more (more than 3 hours)\n",
    "# that is the equivalent of taking half a day off\n",
    "\n",
    "# initial code from the lecture\n",
    "# targets = np.where(data_preprocessed['Absenteeism Time in Hours'] > 3, 1, 0)\n",
    "\n",
    "# parameterized code\n",
    "targets = np.where(data_preprocessed['Absenteeism Time in Hours'] > \n",
    "                   data_preprocessed['Absenteeism Time in Hours'].median(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eyeball the targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Series in the original data frame that will contain the targets for the regression\n",
    "data_preprocessed['Excessive Absenteeism'] = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what happened\n",
    "# maybe manually see how the targets were created\n",
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A comment on the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dataset is balanced (what % of targets are 1s)\n",
    "# targets.sum() will give us the number of 1s that there are\n",
    "# the shape[0] will give us the length of the targets array\n",
    "targets.sum() / targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a checkpoint by dropping the unnecessary variables\n",
    "data_with_targets = data_preprocessed.drop(['Absenteeism Time in Hours'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the line above is a checkpoint :)\n",
    "\n",
    "# if data_with_targets is data_preprocessed = True, then the two are pointing to the same object\n",
    "# if it is False, then the two variables are completely different and this is in fact a checkpoint\n",
    "data_with_targets is data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what's inside\n",
    "data_with_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the inputs for the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selects all rows and all columns until 14 (excluding)\n",
    "data_with_targets.iloc[:,:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selects all rows and all columns but the last one (basically the same operation)\n",
    "data_with_targets.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable that will contain the inputs (everything without the targets)\n",
    "unscaled_inputs = data_with_targets.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the inputs\n",
    "\n",
    "# standardization is one of the most common preprocessing tools\n",
    "# since data of different magnitude (scale) can be biased towards high values,\n",
    "# we want all inputs to be of similar magnitude\n",
    "# this is a peculiarity of machine learning in general - most (but not all) algorithms do badly with unscaled data\n",
    "\n",
    "# a very useful module we can use is StandardScaler \n",
    "# it has much more capabilities than the straightforward 'preprocessing' method\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# we will create a variable that will contain the scaling information for this particular dataset\n",
    "# here's the full documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "# define scaler as an object\n",
    "absenteeism_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the unscaled_inputs\n",
    "# this basically calculates the mean and standard deviation of each feature\n",
    "absenteeism_scaler.fit(unscaled_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the unscaled inputs\n",
    "# this is the scaling itself - we subtract the mean and divide by the standard deviation\n",
    "scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eyeball the newly created variable\n",
    "scaled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the array\n",
    "scaled_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train & test and shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split so we can split our data into train and test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check how this method works\n",
    "train_test_split(scaled_inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare 4 variables for the split\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, #train_size = 0.8, \n",
    "                                                                            test_size = 0.2, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the train inputs and targets\n",
    "print (x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the test inputs and targets\n",
    "print (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LogReg model from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import the 'metrics' module, which includes important metrics we may want to use\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logistic regression object\n",
    "reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit our train inputs\n",
    "# that is basically the whole training part of the machine learning\n",
    "reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the train accuracy of the model\n",
    "reg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find the model outputs according to our model\n",
    "model_outputs = reg.predict(x_train)\n",
    "model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare them with the targets\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ACTUALLY compare the two variables\n",
    "model_outputs == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out in how many instances we predicted correctly\n",
    "np.sum((model_outputs==y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total number of instances\n",
    "model_outputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy of the model\n",
    "# divide the number of correctly predicted outputs by the number of all outputs\n",
    "np.sum((model_outputs==y_train)) / model_outputs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the intercept and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intercept (bias) of our model\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coefficients (weights) of our model\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what were the names of our columns\n",
    "unscaled_inputs.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the names of the columns in an ad-hoc variable\n",
    "feature_name = unscaled_inputs.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the coefficients from this table (they will be exported later and will be used in Tableau)\n",
    "# transpose the model coefficients (model.coef_) and throws them into a df (a vertical organization, so that they can be\n",
    "# multiplied by certain matrices later) \n",
    "summary_table = pd.DataFrame (columns=['Feature name'], data = feature_name)\n",
    "\n",
    "# add the coefficient values to the summary table\n",
    "summary_table['Coefficient'] = np.transpose(reg.coef_)\n",
    "\n",
    "# display the summary table\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do a little Python trick to move the intercept to the top of the summary table\n",
    "# move all indices by 1\n",
    "summary_table.index = summary_table.index + 1\n",
    "\n",
    "# add the intercept at index 0\n",
    "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
    "\n",
    "# sort the df by index\n",
    "summary_table = summary_table.sort_index()\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Series called: 'Odds ratio' which will show the.. odds ratio of each feature\n",
    "summary_table['Odds_ratio'] = np.exp(summary_table.Coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the df\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the table according to odds ratio\n",
    "# note that by default, the sort_values method sorts values by 'ascending'\n",
    "summary_table.sort_values('Odds_ratio', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
